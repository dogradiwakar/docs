{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs \u00b6 For full documentation visit mkdocs.org . Commands \u00b6 mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout \u00b6 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files. Apache Spark \u00b6","title":"Home"},{"location":"#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"#apache-spark","text":"","title":"Apache Spark"},{"location":"GettingStarted/","text":"Downloading Apache Spark \u00b6 Go to https://spark.apache.org/downloads.html and download latest Spark file . This will download all the binaries which will be required to run spark in local mode . In case of an existing Hadoop installation we would need to select Hadoop version from the dropdown . Spark Directories and Files . \u00b6 On extracting the spark tar file , below would be the directories which would be created . \u2022 Readme . md : contains instructions on how to use spark shell , build spark from source & Contains links to documentation \u2022 bin : contains scripts to interact with spark , contains shells and executables \u2022 sbin : contains all the scripts required for administrative purposes such as starting / stopping spark . \u2022 kubernetes : contains docker files for creating docker images for spark distribution on a kubernetes cluster \u2022 data : contains . txt files that serve as input for spark components such as MLib , structured streaming , GraphX etc . \u2022 examples : contains example Java , R , python files . Using the Spark or Pyspark Shell \u00b6 Check the Java installation as it is a pre-requisite run java-version to check Run update & then install Java sudo apt-get update sudo apt-get install openjdk-8-jdk cd into the bin directory and type pyspark to stark the shell . ./pyspark Similarly to start spark-shell run below command ./spark-shell Spark UI Graphical interface used to inspect or monitor spark applications . Runs on http://localhost:4040/","title":"GettingStarted"},{"location":"GettingStarted/#downloading-apache-spark","text":"Go to https://spark.apache.org/downloads.html and download latest Spark file . This will download all the binaries which will be required to run spark in local mode . In case of an existing Hadoop installation we would need to select Hadoop version from the dropdown .","title":"Downloading Apache Spark"},{"location":"GettingStarted/#spark-directories-and-files","text":"On extracting the spark tar file , below would be the directories which would be created . \u2022 Readme . md : contains instructions on how to use spark shell , build spark from source & Contains links to documentation \u2022 bin : contains scripts to interact with spark , contains shells and executables \u2022 sbin : contains all the scripts required for administrative purposes such as starting / stopping spark . \u2022 kubernetes : contains docker files for creating docker images for spark distribution on a kubernetes cluster \u2022 data : contains . txt files that serve as input for spark components such as MLib , structured streaming , GraphX etc . \u2022 examples : contains example Java , R , python files .","title":"Spark Directories and Files ."},{"location":"GettingStarted/#using-the-spark-or-pyspark-shell","text":"Check the Java installation as it is a pre-requisite run java-version to check Run update & then install Java sudo apt-get update sudo apt-get install openjdk-8-jdk cd into the bin directory and type pyspark to stark the shell . ./pyspark Similarly to start spark-shell run below command ./spark-shell Spark UI Graphical interface used to inspect or monitor spark applications . Runs on http://localhost:4040/","title":"Using the Spark or Pyspark Shell"},{"location":"JupyterNotebookIntegration/","text":"Anaconda Installation \u00b6 Download anaconda from below url https://www.anaconda.com/products/individual Install Anaconda bash Anaconda3-2021.05-Linux-x86_64.sh Downloading Apache Spark \u00b6 Go to https://spark.apache.org/downloads.html and download latest Spark file . Unzip it and Update Bash profile: tar -xzf spark-3.1.1-bin-hadoop2.7.tgz ` Configure Path nano ~/.bashrc ` Add below to bash profile export SPARK_HOME = /home/diwakar/Downloads/spark-3.1.1 export PATH = $SPARK_HOME /bin: $PATH Configure PySpark driver \u00b6 nano ~/.bashrc ` Add below to the environment variables export PYSPARK_DRIVER_PYTHON = jupyter export PYSPARK_DRIVER_PYTHON_OPTS = ' notebook Run \"pyspark\" in the window to start the jupyter notebook Run below code to test from pyspark.sql.types import StructType , StructField , FloatType , BooleanType from pyspark.sql.types import DoubleType , IntegerType , StringType import pyspark from pyspark import SQLContext conf = pyspark . SparkConf () sc = pyspark . SparkContext . getOrCreate ( conf = conf ) sqlcontext = SQLContext ( sc ) schema = StructType ([ StructField ( \"sales\" , IntegerType (), True ), StructField ( \"sales person\" , StringType (), True ) ]) data = ([( 10 , 'Walker' ), ( 20 , 'Stepher' ) ]) df = sqlcontext . createDataFrame ( data , schema = schema ) df . show ()","title":"Jupyter Notebook Integration"},{"location":"JupyterNotebookIntegration/#anaconda-installation","text":"Download anaconda from below url https://www.anaconda.com/products/individual Install Anaconda bash Anaconda3-2021.05-Linux-x86_64.sh","title":"Anaconda Installation"},{"location":"JupyterNotebookIntegration/#downloading-apache-spark","text":"Go to https://spark.apache.org/downloads.html and download latest Spark file . Unzip it and Update Bash profile: tar -xzf spark-3.1.1-bin-hadoop2.7.tgz ` Configure Path nano ~/.bashrc ` Add below to bash profile export SPARK_HOME = /home/diwakar/Downloads/spark-3.1.1 export PATH = $SPARK_HOME /bin: $PATH","title":"Downloading Apache Spark"},{"location":"JupyterNotebookIntegration/#configure-pyspark-driver","text":"nano ~/.bashrc ` Add below to the environment variables export PYSPARK_DRIVER_PYTHON = jupyter export PYSPARK_DRIVER_PYTHON_OPTS = ' notebook Run \"pyspark\" in the window to start the jupyter notebook Run below code to test from pyspark.sql.types import StructType , StructField , FloatType , BooleanType from pyspark.sql.types import DoubleType , IntegerType , StringType import pyspark from pyspark import SQLContext conf = pyspark . SparkConf () sc = pyspark . SparkContext . getOrCreate ( conf = conf ) sqlcontext = SQLContext ( sc ) schema = StructType ([ StructField ( \"sales\" , IntegerType (), True ), StructField ( \"sales person\" , StringType (), True ) ]) data = ([( 10 , 'Walker' ), ( 20 , 'Stepher' ) ]) df = sqlcontext . createDataFrame ( data , schema = schema ) df . show ()","title":"Configure PySpark driver"},{"location":"Overview/","text":"Introduction \u00b6 Why Spark??? \u00b6 Hadoop was the first version of distributed computing Issues with Hadoop Not Unified You need to use Hive to write SQL , Mahout for ML , Zookeeper as cluster manager etc Hard to manage and administer Batch processing using Map Reduce Large Datasets were written to disk for subsequent stages of operation which meant high i/o. OK for batch processing but was slow while doing Machine learning / Streaming Spark was created in order to overcome these obstacles . What is Spark? \u00b6 Apache Spark is a unified engine designed for large-scale distributed data processing, on premises in data centers or in the cloud. Spark provides in-memory storage for intermediate computations, making it much faster than Hadoop Map Reduce. It incorporates libraries with composable APIs for machine learning (MLlib), SQL for interactive queries (Spark SQL), stream processing (Structured Streaming) for interacting with real-time data, and graph processing (GraphX). Spark\u2019s design philosophy centers around four key characteristics: Speed - The Spark framework is optimized to benefit from the cheap commodity hardware nowadays (CPU / RAM) - Spark builds its query as Directed acrylic graph (DAG) which constructs an efficient computational graph that can be decomposed into tasks and then can be executed in parallel across the workers in cluster . - As data is retained in memory with limited disk i/o it has a huge performance boost . Ease of use - All the high level abstractions such as data frames/datasets are built on top of simple logical structure called RDD. - This eventually leads to a simple programming model . Modularity - Supports Scala/Java/Python/R - All the libraries are well documented and are unified - We can write a single application to do ML/Streaming in one go . Extensibility - Focuses on parallel computation rather than storage - Decouple storage and computation - Supports many data sources and targets Apache Hadoop, Apache Cassandra, Apache HBase, MongoDB, Apache Hive, RDBMSs, and more\u2014and process it all in memory. - The community of Spark developers maintains a list of third-party Spark packages as part of the growing ecosystem Lets Revisit hadoop issues and compare those with Spark : \u00b6 Not Unified You need to use Hive to write SQL , Mahout for ML , Zookeeper as cluster manager etc -> Spark is unified with all ML, Streaming , SQL functionalities built in the core itself . Hard to manage and administer -> Spark is easy to Manage and administer Batch processing using Map Reduce -> Spark is must faster than traditional map reduce Large Datasets were written to disk for subsequent stages of operation which meant high i/o.-> Spark writes the data in memory making it much faster OK for batch processing but was slow while doing Machine learning / Streaming-> Spark is used for real time streaming and data processing . Apache Spark Components as a Unified Stack \u00b6 Spark has Four major components : Each of these components is separate in spark core engine So in whichever language you write the code Python/R the core decomposes it into highly compact byte code that is executed in worker's JVM across the cluster . Spark SQL: Works Well with Structured Data (RDBMS, csv, parquet , AVRO, ORC) Used to Read Data from RDBMS or structured data (csv, text, json etc) and create permanent / temporary tables in spark. Can be used to read data from dataframe . Useful to run SQL type queries Spark MLib Mlib provides many popular ML algorithms built on top of high level DataFrame based API to build models . These API allow to extract or transform features , build pipelines and persist models during deployment . Spark Structured Streaming Continuous streaming model where a stream of continuous streaming data can be consumed . Developers can treat these streams as tables and query them. Used to combine and react in real time to both static data and streaming data from engines like Apache Kafka , kinesis and other data sources . GraphX Library for manipulating Graphs and perform graph parallel computations . Apache Spark\u2019s Distributed Execution Model \u00b6 Spark Driver : Instantiates a spark Session Maintains information about the spark application. It\u2019s the heart of a Spark Application and maintains all relevant information during the lifetime of the application. It Communicates with cluster manager , requests resources from Cluster Manager for spark execution (JVM) , transforms spark operations into DAG, schedules operations and co-ordinates with executors . Spark Session The Spark application is controlled using SparkSession. Uniform conduit for all spark operations and data Used for creating JVM parameters , define Dataframe , Datasets , read data sources , access catalog metadata and issues SQL queries . Entry point for all Spark functionality Can be accessed using global variable spark or using sc . Cluster Manager Responsible for managing and allocating resources for the cluster Spark supports 4 cluster managers In built Standalone Apache Hadoop Yarn Apache Mesos Kubernetes Spark Executor Runs on each worker node in the cluster Communicates with driver program and execute tasks on the worker nodes They have 2 major responsibilities Execute Code assigned to it by the driver Reporting state of computation on that executor back to the driver node . Distributed Data and Partitions \u00b6 Actual Physical data is stored as partitions. It is a collection of rows that sit on one physical machine in your cluster. Spark treats each partition as a dataframe in memory Spark Executor reads the data from its closest partition Partitioning allows parallelism as Each core in partition is assigned to its own data partition to work with . If you have one partition, Spark will have a parallelism of only one, even if you have thousands of executors. If you have many partitions but only one executor, Spark will still have a parallelism of only one because there is only one computation resource. Partitioning allows efficient parallelism . In distributed environment spark executor reads data from the nearest partition allowing efficient parallelism . For example, this code snippet will break up the physical data stored across clusters into eight partitions, and each executor will get one or more partitions to read into its memory: log_df = spark.read.text(\"path_to_large_text_file\").repartition(8) print(log_df.rdd.getNumPartitions()) Understanding Spark Application Concepts \u00b6 Key terminologies Application : User Program consisting of driver and spark session . Spark Session : Entry point to interact with Spark functionality . Job : Parallel Computation consisting of many tasks Stage : Each Job is divided into smaller tasks called stages Task : Single unit of work Spark Application and Spark Session At the core of Spark Application is the spark driver program which creates a SparkSession object . While working with an interactive shell Spark session is created automatically Spark Jobs When we invoke commands through spark-shell the driver converts the spark application into various spark jobs which in turn converts each job into multiple DAG's . Spark Stages As park of DAG nodes stages re created based on the operations which needs to be performed . Spark Tasks Each stage is comprised of multiple tasks which is a unit of execution which gets federated across the spark executors . Each task maps to a single core and works on a single partition of data . For an executor with 16 core 16 or more tasks would run working on 16 or more partitions in parallel . Transformations Actions and Lazy Executions \u00b6 Transformations transform a spark dataframe into a new dataframe without altering the original data as it is immutable . E.g. a select() or a filter() command will not change the original dataframe but will return a new dataframe. Transformations are the core of how you express your business logic using Spark. All transformations are evaluated lazily aka the results are not computed immediately but are recorded which allows spark to rearrange the transformations , optimize them into stages for efficient execution . Lazy evaluation allows spark to record transformations until an action is invoked . Each transformation produces a new dataframe . Actions : Anything which triggers execution of a transformation like show, count etc. Kind of actions - Actions to view data in the console - Actions to collect data to native objects in the respective language - Actions to write to output data sources - Narrow and Wide Transformations \u00b6 Narrow Transformation : Where a single output is partition can be computed from a single input partition . E.g. filter(), contains() operate on single partition . Wide Transformations : Where a shuffle of partitions happens . E.g. groupby(), orderby() leads data to be read from many partitions combine them and then written to disk . An End-to-End Example \u00b6 We\u2019ll use Spark to analyze some flight data from the United States Bureau of Transportation statistics. Below is how data looks in csv file $ head / data / flight - data / csv / 2015 - summary . csv DEST_COUNTRY_NAME , ORIGIN_COUNTRY_NAME , count United States , Romania , 15 United States , Croatia , 1 United States , Ireland , 344 We will use below code to read the csv into a spark Dataframe flightData2015 = spark \\ . read \\ . option ( \"inferSchema\" , \"true\" ) \\ . option ( \"header\" , \"true\" ) \\ . csv ( \"/data/flight-data/csv/2015-summary.csv\" ) The Dataframe just created has a set of columns with an unspecified number of rows. The reason the number of rows is unspecified is because reading data is a transformation, and is therefore a lazy operation. Spark peeked at only a couple of rows of data to try to guess what types each column should be. Below provides an illustration of the CSV file being read into a Dataframe and then being converted into a local array or list of rows. If we perform the take action on the Dataframe, we will be able to see the same results that we saw before when we used the command line: flightData2015.take(3) Array([United States,Romania,15], [United States,Croatia... Let\u2019s specify some more transformations! Now, let\u2019s sort our data according to the count column, which is an integer type. Below figure illustrates this process. Also Sort does not modify the DataFrame. We use sort as a transformation that returns a new DataFrame by transforming the previous DataFrame. Nothing happens to the data when we call sort because it\u2019s just a transformation. However, we can see that Spark is building up a plan for how it will execute this across the cluster by looking at the explain plan. We can call explain on any DataFrame object to see the DataFrame\u2019s lineage (or how Spark will execute this query): flightData2015.sort(\"count\").explain() == Physical Plan == *Sort [count#195 ASC NULLS FIRST], true, 0 +- Exchange rangepartitioning(count#195 ASC NULLS FIRST, 200) +- *FileScan csv [DEST_COUNTRY_NAME#193,ORIGIN_COUNTRY_NAME#194,count#195] ... You can read explain plans from top to bottom, the top being the end result, and the bottom being the source(s) of data. In this case, take a look at the first keywords. You will see sort, exchange, and FileScan. That\u2019s because the sort of our data is actually a wide transformation because rows will need to be compared with one another. Now, just like we did before, we can specify an action to kick off this plan. However, before doing that, we\u2019re going to set a configuration. By default, when we perform a shuffle, Spark outputs 200 shuffle partitions. Let\u2019s set this value to 5 to reduce the number of the output partitions from the shuffle: spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\") flightData2015.sort(\"count\").take(2) ... Array([United States,Singapore,1], [Moldova,United States,1]) Below figure illustrates this operation. Notice that in addition to the logical transformations, we include the physical partition count, as well. The logical plan of transformations that we build up defines a lineage for the DataFrame so that at any given point in time, Spark knows how to recompute any partition by performing all of the operations it had before on the same input data. This sits at the heart of Spark\u2019s programming model\u2014functional programming where the same inputs always result in the same outputs when the transformations on that data stay constant. We do not manipulate the physical data; instead, we configure physical execution characteristics through things like the shuffle partitions parameter . We ended up with five output partitions because that\u2019s the value we specified in the shuffle partition. You can change this to help control the physical execution characteristics of your Spark jobs. In experimenting with different values, you should see drastically different runtimes. You can monitor the job progress by navigating to the Spark UI on port 4040 to see the physical and logical execution characteristics of your jobs.","title":"Overview"},{"location":"Overview/#introduction","text":"","title":"Introduction"},{"location":"Overview/#why-spark","text":"Hadoop was the first version of distributed computing Issues with Hadoop Not Unified You need to use Hive to write SQL , Mahout for ML , Zookeeper as cluster manager etc Hard to manage and administer Batch processing using Map Reduce Large Datasets were written to disk for subsequent stages of operation which meant high i/o. OK for batch processing but was slow while doing Machine learning / Streaming Spark was created in order to overcome these obstacles .","title":"Why Spark???"},{"location":"Overview/#what-is-spark","text":"Apache Spark is a unified engine designed for large-scale distributed data processing, on premises in data centers or in the cloud. Spark provides in-memory storage for intermediate computations, making it much faster than Hadoop Map Reduce. It incorporates libraries with composable APIs for machine learning (MLlib), SQL for interactive queries (Spark SQL), stream processing (Structured Streaming) for interacting with real-time data, and graph processing (GraphX). Spark\u2019s design philosophy centers around four key characteristics: Speed - The Spark framework is optimized to benefit from the cheap commodity hardware nowadays (CPU / RAM) - Spark builds its query as Directed acrylic graph (DAG) which constructs an efficient computational graph that can be decomposed into tasks and then can be executed in parallel across the workers in cluster . - As data is retained in memory with limited disk i/o it has a huge performance boost . Ease of use - All the high level abstractions such as data frames/datasets are built on top of simple logical structure called RDD. - This eventually leads to a simple programming model . Modularity - Supports Scala/Java/Python/R - All the libraries are well documented and are unified - We can write a single application to do ML/Streaming in one go . Extensibility - Focuses on parallel computation rather than storage - Decouple storage and computation - Supports many data sources and targets Apache Hadoop, Apache Cassandra, Apache HBase, MongoDB, Apache Hive, RDBMSs, and more\u2014and process it all in memory. - The community of Spark developers maintains a list of third-party Spark packages as part of the growing ecosystem","title":"What is Spark?"},{"location":"Overview/#lets-revisit-hadoop-issues-and-compare-those-with-spark","text":"Not Unified You need to use Hive to write SQL , Mahout for ML , Zookeeper as cluster manager etc -> Spark is unified with all ML, Streaming , SQL functionalities built in the core itself . Hard to manage and administer -> Spark is easy to Manage and administer Batch processing using Map Reduce -> Spark is must faster than traditional map reduce Large Datasets were written to disk for subsequent stages of operation which meant high i/o.-> Spark writes the data in memory making it much faster OK for batch processing but was slow while doing Machine learning / Streaming-> Spark is used for real time streaming and data processing .","title":"Lets Revisit hadoop issues and compare those with Spark :"},{"location":"Overview/#apache-spark-components-as-a-unified-stack","text":"Spark has Four major components : Each of these components is separate in spark core engine So in whichever language you write the code Python/R the core decomposes it into highly compact byte code that is executed in worker's JVM across the cluster . Spark SQL: Works Well with Structured Data (RDBMS, csv, parquet , AVRO, ORC) Used to Read Data from RDBMS or structured data (csv, text, json etc) and create permanent / temporary tables in spark. Can be used to read data from dataframe . Useful to run SQL type queries Spark MLib Mlib provides many popular ML algorithms built on top of high level DataFrame based API to build models . These API allow to extract or transform features , build pipelines and persist models during deployment . Spark Structured Streaming Continuous streaming model where a stream of continuous streaming data can be consumed . Developers can treat these streams as tables and query them. Used to combine and react in real time to both static data and streaming data from engines like Apache Kafka , kinesis and other data sources . GraphX Library for manipulating Graphs and perform graph parallel computations .","title":"Apache Spark Components as a Unified Stack"},{"location":"Overview/#apache-sparks-distributed-execution-model","text":"Spark Driver : Instantiates a spark Session Maintains information about the spark application. It\u2019s the heart of a Spark Application and maintains all relevant information during the lifetime of the application. It Communicates with cluster manager , requests resources from Cluster Manager for spark execution (JVM) , transforms spark operations into DAG, schedules operations and co-ordinates with executors . Spark Session The Spark application is controlled using SparkSession. Uniform conduit for all spark operations and data Used for creating JVM parameters , define Dataframe , Datasets , read data sources , access catalog metadata and issues SQL queries . Entry point for all Spark functionality Can be accessed using global variable spark or using sc . Cluster Manager Responsible for managing and allocating resources for the cluster Spark supports 4 cluster managers In built Standalone Apache Hadoop Yarn Apache Mesos Kubernetes Spark Executor Runs on each worker node in the cluster Communicates with driver program and execute tasks on the worker nodes They have 2 major responsibilities Execute Code assigned to it by the driver Reporting state of computation on that executor back to the driver node .","title":"Apache Spark\u2019s Distributed Execution Model"},{"location":"Overview/#distributed-data-and-partitions","text":"Actual Physical data is stored as partitions. It is a collection of rows that sit on one physical machine in your cluster. Spark treats each partition as a dataframe in memory Spark Executor reads the data from its closest partition Partitioning allows parallelism as Each core in partition is assigned to its own data partition to work with . If you have one partition, Spark will have a parallelism of only one, even if you have thousands of executors. If you have many partitions but only one executor, Spark will still have a parallelism of only one because there is only one computation resource. Partitioning allows efficient parallelism . In distributed environment spark executor reads data from the nearest partition allowing efficient parallelism . For example, this code snippet will break up the physical data stored across clusters into eight partitions, and each executor will get one or more partitions to read into its memory: log_df = spark.read.text(\"path_to_large_text_file\").repartition(8) print(log_df.rdd.getNumPartitions())","title":"Distributed Data and Partitions"},{"location":"Overview/#understanding-spark-application-concepts","text":"Key terminologies Application : User Program consisting of driver and spark session . Spark Session : Entry point to interact with Spark functionality . Job : Parallel Computation consisting of many tasks Stage : Each Job is divided into smaller tasks called stages Task : Single unit of work Spark Application and Spark Session At the core of Spark Application is the spark driver program which creates a SparkSession object . While working with an interactive shell Spark session is created automatically Spark Jobs When we invoke commands through spark-shell the driver converts the spark application into various spark jobs which in turn converts each job into multiple DAG's . Spark Stages As park of DAG nodes stages re created based on the operations which needs to be performed . Spark Tasks Each stage is comprised of multiple tasks which is a unit of execution which gets federated across the spark executors . Each task maps to a single core and works on a single partition of data . For an executor with 16 core 16 or more tasks would run working on 16 or more partitions in parallel .","title":"Understanding Spark Application Concepts"},{"location":"Overview/#transformations-actions-and-lazy-executions","text":"Transformations transform a spark dataframe into a new dataframe without altering the original data as it is immutable . E.g. a select() or a filter() command will not change the original dataframe but will return a new dataframe. Transformations are the core of how you express your business logic using Spark. All transformations are evaluated lazily aka the results are not computed immediately but are recorded which allows spark to rearrange the transformations , optimize them into stages for efficient execution . Lazy evaluation allows spark to record transformations until an action is invoked . Each transformation produces a new dataframe . Actions : Anything which triggers execution of a transformation like show, count etc. Kind of actions - Actions to view data in the console - Actions to collect data to native objects in the respective language - Actions to write to output data sources -","title":"Transformations Actions and Lazy Executions"},{"location":"Overview/#narrow-and-wide-transformations","text":"Narrow Transformation : Where a single output is partition can be computed from a single input partition . E.g. filter(), contains() operate on single partition . Wide Transformations : Where a shuffle of partitions happens . E.g. groupby(), orderby() leads data to be read from many partitions combine them and then written to disk .","title":"Narrow and Wide Transformations"},{"location":"Overview/#an-end-to-end-example","text":"We\u2019ll use Spark to analyze some flight data from the United States Bureau of Transportation statistics. Below is how data looks in csv file $ head / data / flight - data / csv / 2015 - summary . csv DEST_COUNTRY_NAME , ORIGIN_COUNTRY_NAME , count United States , Romania , 15 United States , Croatia , 1 United States , Ireland , 344 We will use below code to read the csv into a spark Dataframe flightData2015 = spark \\ . read \\ . option ( \"inferSchema\" , \"true\" ) \\ . option ( \"header\" , \"true\" ) \\ . csv ( \"/data/flight-data/csv/2015-summary.csv\" ) The Dataframe just created has a set of columns with an unspecified number of rows. The reason the number of rows is unspecified is because reading data is a transformation, and is therefore a lazy operation. Spark peeked at only a couple of rows of data to try to guess what types each column should be. Below provides an illustration of the CSV file being read into a Dataframe and then being converted into a local array or list of rows. If we perform the take action on the Dataframe, we will be able to see the same results that we saw before when we used the command line: flightData2015.take(3) Array([United States,Romania,15], [United States,Croatia... Let\u2019s specify some more transformations! Now, let\u2019s sort our data according to the count column, which is an integer type. Below figure illustrates this process. Also Sort does not modify the DataFrame. We use sort as a transformation that returns a new DataFrame by transforming the previous DataFrame. Nothing happens to the data when we call sort because it\u2019s just a transformation. However, we can see that Spark is building up a plan for how it will execute this across the cluster by looking at the explain plan. We can call explain on any DataFrame object to see the DataFrame\u2019s lineage (or how Spark will execute this query): flightData2015.sort(\"count\").explain() == Physical Plan == *Sort [count#195 ASC NULLS FIRST], true, 0 +- Exchange rangepartitioning(count#195 ASC NULLS FIRST, 200) +- *FileScan csv [DEST_COUNTRY_NAME#193,ORIGIN_COUNTRY_NAME#194,count#195] ... You can read explain plans from top to bottom, the top being the end result, and the bottom being the source(s) of data. In this case, take a look at the first keywords. You will see sort, exchange, and FileScan. That\u2019s because the sort of our data is actually a wide transformation because rows will need to be compared with one another. Now, just like we did before, we can specify an action to kick off this plan. However, before doing that, we\u2019re going to set a configuration. By default, when we perform a shuffle, Spark outputs 200 shuffle partitions. Let\u2019s set this value to 5 to reduce the number of the output partitions from the shuffle: spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\") flightData2015.sort(\"count\").take(2) ... Array([United States,Singapore,1], [Moldova,United States,1]) Below figure illustrates this operation. Notice that in addition to the logical transformations, we include the physical partition count, as well. The logical plan of transformations that we build up defines a lineage for the DataFrame so that at any given point in time, Spark knows how to recompute any partition by performing all of the operations it had before on the same input data. This sits at the heart of Spark\u2019s programming model\u2014functional programming where the same inputs always result in the same outputs when the transformations on that data stay constant. We do not manipulate the physical data; instead, we configure physical execution characteristics through things like the shuffle partitions parameter . We ended up with five output partitions because that\u2019s the value we specified in the shuffle partition. You can change this to help control the physical execution characteristics of your Spark jobs. In experimenting with different values, you should see drastically different runtimes. You can monitor the job progress by navigating to the Spark UI on port 4040 to see the physical and logical execution characteristics of your jobs.","title":"An End-to-End Example"},{"location":"Secure%20Data%20Sharing%20SnowFlake%2070f8b90e87be44ac86db45481ea930cc/","text":"Secure Data Sharing SnowFlake \u00b6 1. Architecture & Key Concepts \u00b6 Snowflake provides below ways to share data with end users. Data Marketplace: \u00b6 Provides capability to discover and access third party data sources. Through Snowflake Marketplace, you can monetize the data if there is a demand for the data. Data Exchange: \u00b6 Provides ability to create a data hub for securely collaborating around data between a selected group of members that you invite. It enables providers to publish data that can then be discovered by consumers. Secure Data Sharing \u00b6 Secure Data Sharing enables sharing selected objects in a database in your account with other Snowflake accounts. The following Snowflake database objects can be shared: Tables External tables Secure views Secure materialized views Secure UDFs With Secure Data Sharing, no actual data is copied or transferred between accounts. All sharing is accomplished through Snowflake\u2019s services layer and metadata store. The shared data does not take up any storage in a consumer account and, therefore, does not contribute to the consumer\u2019s monthly data storage charges. The only charges to consumers are for the compute resources (i.e. virtual warehouses) used to query the shared data. Flow The provider creates a share of a database in their account and grants access to specific objects in the database. On the consumer side, a read-only database is created from the share. Shares Shares are named Snowflake objects that encapsulate all of the information required to share a database. Each share consists of: The privileges that grant access to the database(s) and the schema containing the objects to share. The privileges that grant access to the specific objects in the database. The consumer accounts with which the database and its objects are shared. Once a database is created (in a consumer account) from a share, all the shared objects are accessible to users in the consumer account: Shares are secure, configurable, and controlled 100% by the provider account: New objects added to a share become immediately available to all consumers, providing real-time access to shared data. Access to a share (or any of the objects in a share) can be revoked at any time. Types of Accounts \u00b6 Providers \u00b6 A data provider is any Snowflake account that creates shares and makes them available to other Snowflake accounts to consume. As a data provider, you share a database with one or more Snowflake accounts. For each database you share, Snowflake supports using grants to provide granular access control to selected objects in the database Consumers \u00b6 A data consumer is any account that chooses to create a database from a share made available by a data provider. As a data consumer, once you add a shared database to your account, you can access and query the objects in the database just as you would with any other database in your account. Reader Accounts \u00b6 Data sharing is only supported between Snowflake accounts. In Order to share data with a consumer who does not already have a Snowflake account a Reader account is used. Each reader account belongs to the provider account that created it. Similar to standard consumer accounts, the provider account uses shares to share databases with reader accounts; however, a reader account can only consume data from the provider account that created it: Users in a reader account can query data that has been shared with it, but cannot perform any of the DML tasks that are allowed in a full account (data loading, insert, update, etc.). 2. Secure Direct Data Share \u00b6 Below is the process to share data using Secure Data Sharing Lets try to share the EMP table from SAMPLE_DB Database Configuration at Provider End \u00b6 2a Create Share \u00b6 Use CREATE SHARE to create a share. At this step, the share is simply a container waiting for objects and accounts to be added. use role accountadmin ; create share CustData ; 2b Add Objects to the share by granting privileges \u00b6 Use GRANT \u2026 TO SHARE to grant the following object privileges to the share: USAGE privilege on the database you wish to share. USAGE privilege on each database schema containing the objects you wish to share. SELECT privilege for sharing specific objects in each shared schema: Tables External tables Secure views Secure materialized views Secure UDFs grant usage on database SAMPLE_DB to share CustData ; grant usage on schema SAMPLE_DB . PUBLIC to share CustData ; grant select on table SAMPLE_DB . PUBLIC . EMP to share CustData ; Run below command to see the grants on the share show grants to share CustData; 2c Sharing Data using Secure View \u00b6 Create a view and grant access to the share Create or replace View SAMPLE_DB . public . EMP_View as Select EMPIDID , NAME from \"SAMPLE_DB\" . \"PUBLIC\" . \"EMP\" ; Try granting access on the view Grant Select on view SAMPLE_DB . public . EMP_View to share CustData ; It will throw an error mentioning a view cannot be shared and only a secure view can be shared . Reason being for a view created as a secure view the view definition cannot be exposed and the consumer cannot see the definition of the view . For this we need to create a secure view . Execute below commands to create a secure view and allow grant on it Create or replace Secure View SAMPLE_DB . public . EMP_View_secure as Select EMPIDID , NAME from \"SAMPLE_DB\" . \"PUBLIC\" . \"EMP\" ; Grant Select on view SAMPLE_DB . public . EMP_View_Secure to share CustData ; Secure views are good way to impose data security for consumers . 2d Add Accounts to the share (one or many) \u00b6 Use ALTER SHARE to add one or more accounts to the share. To review the accounts added to the share, you can use SHOW GRANTS. Multiple accounts should be in comma separated format . alter share CustData add accounts = bo94519 ; Once done the access to shares can be checked from Shares \u2192 Outbound tab . Configuration at Consumer End \u00b6 At Consumer end under Shares \u2192 Inbound the new secure share name will show up Run below commands on the Snowflake account which is the consumer CREATE DATABASE \"CUSTDATA_SHARED\" FROM SHARE VH09452 . \"CUSTDATA\" ; GRANT IMPORTED PRIVILEGES ON DATABASE \"CUSTDATA_SHARED\" TO ROLE \"ACCOUNTADMIN\" ; The new Database will show up and can be queries as a normal Database . Revoking access to Share \u00b6 --Revoke Access Alter Share CustData remove account = bo955dd19 ; Revoke select on table SAMPLE_DB . PUBLIC . EMP from share CustData ; Drop share CustData ; 3. Creating a Reader Account and adding access \u00b6 Reader account is created when end user / consumer doesn't have a snowflake account . To create a Reader Account Go To \u2192 Account\u2192 Reader \u2192 \u201cCreate Reader Account\u201d. All the Processing and Data cost are billed to provider in case of a reader account . Once created a unique account url will be generated and can be shared with the Reader account user . 3b Configuring Reader Account Access. \u00b6 Create a share and add objects access to the share use role accountadmin ; create share CustData ; grant usage on database SAMPLE_DB to share CustData ; grant usage on schema SAMPLE_DB . PUBLIC to share CustData ; grant select on table SAMPLE_DB . PUBLIC . EMP to share CustData ; Once the Reader logs into his account he will not be able to see anything Grant access to the share ALTER SHARE \"CUSTDATA\" ADD ACCOUNTS = AL20209 ; 3c Configuration at Reader End \u00b6 On Reader end the share would be visible under Shares \u2192 Inbound Tab Run below command to create a database from share CREATE DATABASE \"CUSTDATA_SHARED\" FROM SHARE VH09468 . \"CUSTDATA\" ; The database and viwes would be visible to the reader account but the query will not work as the due to non avilability of virtual warehouse on Reader account Create a new warehouse and then the query should work fine CREATE WAREHOUSE WH_Reader WITH WAREHOUSE_SIZE = 'XSMALL' WAREHOUSE_TYPE = 'STANDARD' AUTO_SUSPEND = 300 AUTO_RESUME = TRUE MIN_CLUSTER_COUNT = 1 MAX_CLUSTER_COUNT = 1 SCALING_POLICY = 'STANDARD' ; 4 Sharing Scenarios \u00b6 4a Data Sharing in Same Cloud Platform within same region with another Snowflake Account \u00b6 Applies to data sharing in same region (for example: US-WEST-2, Oregon) where Provider pays for Storage and Consumer pays for Compute Solution : Create a Share and add objects to the Share , Provide access to the Consumer . 4b Data Sharing in Same Cloud, Same Region with Reader Account for Non-Snowflake Customers \u00b6 Applies for data sharing in Same region with Non Snowflake Customers. Provider pays for Storage ,Provider also pays for Compute and Manages a Reader Account for Consumer Solution : Create a Reader Account ,Create a Share and add objects to the Share , Provide access to the Reader to the Share. 4c Data Sharing in Different Cloud Platform or Same Cloud Platform , but different Region with another Snowflake Account \u00b6 Applies for data sharing between different Cloud Platform or Different Regions within same Cloud platform, Provider pays for Storage ,Provider pays for Data Replication Costs and Consumer pays for Compute Solution : Set up Data Replication and Share data using Secure Share . https://docs.snowflake.com/en/user-guide/secure-data-sharing-across-regions-plaforms.html","title":"Overview"},{"location":"Secure%20Data%20Sharing%20SnowFlake%2070f8b90e87be44ac86db45481ea930cc/#secure-data-sharing-snowflake","text":"","title":"Secure Data Sharing SnowFlake"},{"location":"Secure%20Data%20Sharing%20SnowFlake%2070f8b90e87be44ac86db45481ea930cc/#1-architecture-key-concepts","text":"Snowflake provides below ways to share data with end users.","title":"1. Architecture &amp; Key Concepts"},{"location":"Secure%20Data%20Sharing%20SnowFlake%2070f8b90e87be44ac86db45481ea930cc/#data-marketplace","text":"Provides capability to discover and access third party data sources. Through Snowflake Marketplace, you can monetize the data if there is a demand for the data.","title":"Data Marketplace:"},{"location":"Secure%20Data%20Sharing%20SnowFlake%2070f8b90e87be44ac86db45481ea930cc/#data-exchange","text":"Provides ability to create a data hub for securely collaborating around data between a selected group of members that you invite. It enables providers to publish data that can then be discovered by consumers.","title":"Data Exchange:"},{"location":"Secure%20Data%20Sharing%20SnowFlake%2070f8b90e87be44ac86db45481ea930cc/#secure-data-sharing","text":"Secure Data Sharing enables sharing selected objects in a database in your account with other Snowflake accounts. The following Snowflake database objects can be shared: Tables External tables Secure views Secure materialized views Secure UDFs With Secure Data Sharing, no actual data is copied or transferred between accounts. All sharing is accomplished through Snowflake\u2019s services layer and metadata store. The shared data does not take up any storage in a consumer account and, therefore, does not contribute to the consumer\u2019s monthly data storage charges. The only charges to consumers are for the compute resources (i.e. virtual warehouses) used to query the shared data. Flow The provider creates a share of a database in their account and grants access to specific objects in the database. On the consumer side, a read-only database is created from the share. Shares Shares are named Snowflake objects that encapsulate all of the information required to share a database. Each share consists of: The privileges that grant access to the database(s) and the schema containing the objects to share. The privileges that grant access to the specific objects in the database. The consumer accounts with which the database and its objects are shared. Once a database is created (in a consumer account) from a share, all the shared objects are accessible to users in the consumer account: Shares are secure, configurable, and controlled 100% by the provider account: New objects added to a share become immediately available to all consumers, providing real-time access to shared data. Access to a share (or any of the objects in a share) can be revoked at any time.","title":"Secure Data Sharing"},{"location":"Secure%20Data%20Sharing%20SnowFlake%2070f8b90e87be44ac86db45481ea930cc/#types-of-accounts","text":"","title":"Types of Accounts"},{"location":"Secure%20Data%20Sharing%20SnowFlake%2070f8b90e87be44ac86db45481ea930cc/#providers","text":"A data provider is any Snowflake account that creates shares and makes them available to other Snowflake accounts to consume. As a data provider, you share a database with one or more Snowflake accounts. For each database you share, Snowflake supports using grants to provide granular access control to selected objects in the database","title":"Providers"},{"location":"Secure%20Data%20Sharing%20SnowFlake%2070f8b90e87be44ac86db45481ea930cc/#consumers","text":"A data consumer is any account that chooses to create a database from a share made available by a data provider. As a data consumer, once you add a shared database to your account, you can access and query the objects in the database just as you would with any other database in your account.","title":"Consumers"},{"location":"Secure%20Data%20Sharing%20SnowFlake%2070f8b90e87be44ac86db45481ea930cc/#reader-accounts","text":"Data sharing is only supported between Snowflake accounts. In Order to share data with a consumer who does not already have a Snowflake account a Reader account is used. Each reader account belongs to the provider account that created it. Similar to standard consumer accounts, the provider account uses shares to share databases with reader accounts; however, a reader account can only consume data from the provider account that created it: Users in a reader account can query data that has been shared with it, but cannot perform any of the DML tasks that are allowed in a full account (data loading, insert, update, etc.).","title":"Reader Accounts"},{"location":"Secure%20Data%20Sharing%20SnowFlake%2070f8b90e87be44ac86db45481ea930cc/#2-secure-direct-data-share","text":"Below is the process to share data using Secure Data Sharing Lets try to share the EMP table from SAMPLE_DB Database","title":"2. Secure Direct Data Share"},{"location":"Secure%20Data%20Sharing%20SnowFlake%2070f8b90e87be44ac86db45481ea930cc/#configuration-at-provider-end","text":"","title":"Configuration at Provider End"},{"location":"Secure%20Data%20Sharing%20SnowFlake%2070f8b90e87be44ac86db45481ea930cc/#2a-create-share","text":"Use CREATE SHARE to create a share. At this step, the share is simply a container waiting for objects and accounts to be added. use role accountadmin ; create share CustData ;","title":"2a Create Share"},{"location":"Secure%20Data%20Sharing%20SnowFlake%2070f8b90e87be44ac86db45481ea930cc/#2b-add-objects-to-the-share-by-granting-privileges","text":"Use GRANT \u2026 TO SHARE to grant the following object privileges to the share: USAGE privilege on the database you wish to share. USAGE privilege on each database schema containing the objects you wish to share. SELECT privilege for sharing specific objects in each shared schema: Tables External tables Secure views Secure materialized views Secure UDFs grant usage on database SAMPLE_DB to share CustData ; grant usage on schema SAMPLE_DB . PUBLIC to share CustData ; grant select on table SAMPLE_DB . PUBLIC . EMP to share CustData ; Run below command to see the grants on the share show grants to share CustData;","title":"2b Add Objects to the share by granting privileges"},{"location":"Secure%20Data%20Sharing%20SnowFlake%2070f8b90e87be44ac86db45481ea930cc/#2c-sharing-data-using-secure-view","text":"Create a view and grant access to the share Create or replace View SAMPLE_DB . public . EMP_View as Select EMPIDID , NAME from \"SAMPLE_DB\" . \"PUBLIC\" . \"EMP\" ; Try granting access on the view Grant Select on view SAMPLE_DB . public . EMP_View to share CustData ; It will throw an error mentioning a view cannot be shared and only a secure view can be shared . Reason being for a view created as a secure view the view definition cannot be exposed and the consumer cannot see the definition of the view . For this we need to create a secure view . Execute below commands to create a secure view and allow grant on it Create or replace Secure View SAMPLE_DB . public . EMP_View_secure as Select EMPIDID , NAME from \"SAMPLE_DB\" . \"PUBLIC\" . \"EMP\" ; Grant Select on view SAMPLE_DB . public . EMP_View_Secure to share CustData ; Secure views are good way to impose data security for consumers .","title":"2c Sharing Data using Secure View"},{"location":"Secure%20Data%20Sharing%20SnowFlake%2070f8b90e87be44ac86db45481ea930cc/#2d-add-accounts-to-the-share-one-or-many","text":"Use ALTER SHARE to add one or more accounts to the share. To review the accounts added to the share, you can use SHOW GRANTS. Multiple accounts should be in comma separated format . alter share CustData add accounts = bo94519 ; Once done the access to shares can be checked from Shares \u2192 Outbound tab .","title":"2d Add Accounts to the share (one or many)"},{"location":"Secure%20Data%20Sharing%20SnowFlake%2070f8b90e87be44ac86db45481ea930cc/#configuration-at-consumer-end","text":"At Consumer end under Shares \u2192 Inbound the new secure share name will show up Run below commands on the Snowflake account which is the consumer CREATE DATABASE \"CUSTDATA_SHARED\" FROM SHARE VH09452 . \"CUSTDATA\" ; GRANT IMPORTED PRIVILEGES ON DATABASE \"CUSTDATA_SHARED\" TO ROLE \"ACCOUNTADMIN\" ; The new Database will show up and can be queries as a normal Database .","title":"Configuration at Consumer End"},{"location":"Secure%20Data%20Sharing%20SnowFlake%2070f8b90e87be44ac86db45481ea930cc/#revoking-access-to-share","text":"--Revoke Access Alter Share CustData remove account = bo955dd19 ; Revoke select on table SAMPLE_DB . PUBLIC . EMP from share CustData ; Drop share CustData ;","title":"Revoking access to Share"},{"location":"Secure%20Data%20Sharing%20SnowFlake%2070f8b90e87be44ac86db45481ea930cc/#3-creating-a-reader-account-and-adding-access","text":"Reader account is created when end user / consumer doesn't have a snowflake account . To create a Reader Account Go To \u2192 Account\u2192 Reader \u2192 \u201cCreate Reader Account\u201d. All the Processing and Data cost are billed to provider in case of a reader account . Once created a unique account url will be generated and can be shared with the Reader account user .","title":"3. Creating a Reader Account and adding access"},{"location":"Secure%20Data%20Sharing%20SnowFlake%2070f8b90e87be44ac86db45481ea930cc/#3b-configuring-reader-account-access","text":"Create a share and add objects access to the share use role accountadmin ; create share CustData ; grant usage on database SAMPLE_DB to share CustData ; grant usage on schema SAMPLE_DB . PUBLIC to share CustData ; grant select on table SAMPLE_DB . PUBLIC . EMP to share CustData ; Once the Reader logs into his account he will not be able to see anything Grant access to the share ALTER SHARE \"CUSTDATA\" ADD ACCOUNTS = AL20209 ;","title":"3b Configuring Reader Account Access."},{"location":"Secure%20Data%20Sharing%20SnowFlake%2070f8b90e87be44ac86db45481ea930cc/#3c-configuration-at-reader-end","text":"On Reader end the share would be visible under Shares \u2192 Inbound Tab Run below command to create a database from share CREATE DATABASE \"CUSTDATA_SHARED\" FROM SHARE VH09468 . \"CUSTDATA\" ; The database and viwes would be visible to the reader account but the query will not work as the due to non avilability of virtual warehouse on Reader account Create a new warehouse and then the query should work fine CREATE WAREHOUSE WH_Reader WITH WAREHOUSE_SIZE = 'XSMALL' WAREHOUSE_TYPE = 'STANDARD' AUTO_SUSPEND = 300 AUTO_RESUME = TRUE MIN_CLUSTER_COUNT = 1 MAX_CLUSTER_COUNT = 1 SCALING_POLICY = 'STANDARD' ;","title":"3c Configuration at Reader End"},{"location":"Secure%20Data%20Sharing%20SnowFlake%2070f8b90e87be44ac86db45481ea930cc/#4-sharing-scenarios","text":"","title":"4 Sharing Scenarios"},{"location":"Secure%20Data%20Sharing%20SnowFlake%2070f8b90e87be44ac86db45481ea930cc/#4a-data-sharing-in-same-cloud-platform-within-same-region-with-another-snowflake-account","text":"Applies to data sharing in same region (for example: US-WEST-2, Oregon) where Provider pays for Storage and Consumer pays for Compute Solution : Create a Share and add objects to the Share , Provide access to the Consumer .","title":"4a Data Sharing in Same Cloud Platform within same region with another Snowflake Account"},{"location":"Secure%20Data%20Sharing%20SnowFlake%2070f8b90e87be44ac86db45481ea930cc/#4b-data-sharing-in-same-cloud-same-region-with-reader-account-for-non-snowflake-customers","text":"Applies for data sharing in Same region with Non Snowflake Customers. Provider pays for Storage ,Provider also pays for Compute and Manages a Reader Account for Consumer Solution : Create a Reader Account ,Create a Share and add objects to the Share , Provide access to the Reader to the Share.","title":"4b Data Sharing in Same Cloud, Same Region with Reader Account for Non-Snowflake Customers"},{"location":"Secure%20Data%20Sharing%20SnowFlake%2070f8b90e87be44ac86db45481ea930cc/#4c-data-sharing-in-different-cloud-platform-or-same-cloud-platform-but-different-region-with-another-snowflake-account","text":"Applies for data sharing between different Cloud Platform or Different Regions within same Cloud platform, Provider pays for Storage ,Provider pays for Data Replication Costs and Consumer pays for Compute Solution : Set up Data Replication and Share data using Secure Share . https://docs.snowflake.com/en/user-guide/secure-data-sharing-across-regions-plaforms.html","title":"4c Data Sharing in Different Cloud Platform or Same Cloud Platform , but different Region with another Snowflake Account"},{"location":"about/","text":"\"# docs\"","title":"About"}]}