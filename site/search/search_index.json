{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs \u00b6 For full documentation visit mkdocs.org . Commands \u00b6 mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout \u00b6 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Welcome to MkDocs"},{"location":"#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"GettingStarted/","text":"Downloading Apache Spark \u00b6 Go to https://spark.apache.org/downloads.html and download latest Spark file . This will download all the binaries which will be required to run spark in local mode . In case of an existing Hadoop installation we would need to select Hadoop version from the dropdown . Spark Directories and Files . \u00b6 On extracting the spark tar file , below would be the directories which would be created . \u2022 Readme . md : contains instructions on how to use spark shell , build spark from source & Contains links to documentation \u2022 bin : contains scripts to interact with spark , contains shells and executables \u2022 sbin : contains all the scripts required for administrative purposes such as starting / stopping spark . \u2022 kubernetes : contains docker files for creating docker images for spark distribution on a kubernetes cluster \u2022 data : contains . txt files that serve as input for spark components such as MLib , structured streaming , GraphX etc . \u2022 examples : contains example Java , R , python files . Using the Spark or Pyspark Shell \u00b6 Check the Java installation as it is a pre-requisite run java-version to check Run update & then install Java sudo apt-get update sudo apt-get install openjdk-8-jdk cd into the bin directory and type pyspark to stark the shell . ./pyspark Similarly to start spark-shell run below command ./spark-shell Spark UI Graphical interface used to inspect or monitor spark applications . Runs on http://localhost:4040/","title":"GettingStarted"},{"location":"GettingStarted/#downloading-apache-spark","text":"Go to https://spark.apache.org/downloads.html and download latest Spark file . This will download all the binaries which will be required to run spark in local mode . In case of an existing Hadoop installation we would need to select Hadoop version from the dropdown .","title":"Downloading Apache Spark"},{"location":"GettingStarted/#spark-directories-and-files","text":"On extracting the spark tar file , below would be the directories which would be created . \u2022 Readme . md : contains instructions on how to use spark shell , build spark from source & Contains links to documentation \u2022 bin : contains scripts to interact with spark , contains shells and executables \u2022 sbin : contains all the scripts required for administrative purposes such as starting / stopping spark . \u2022 kubernetes : contains docker files for creating docker images for spark distribution on a kubernetes cluster \u2022 data : contains . txt files that serve as input for spark components such as MLib , structured streaming , GraphX etc . \u2022 examples : contains example Java , R , python files .","title":"Spark Directories and Files ."},{"location":"GettingStarted/#using-the-spark-or-pyspark-shell","text":"Check the Java installation as it is a pre-requisite run java-version to check Run update & then install Java sudo apt-get update sudo apt-get install openjdk-8-jdk cd into the bin directory and type pyspark to stark the shell . ./pyspark Similarly to start spark-shell run below command ./spark-shell Spark UI Graphical interface used to inspect or monitor spark applications . Runs on http://localhost:4040/","title":"Using the Spark or Pyspark Shell"},{"location":"JupyterNotebookIntegration/","text":"Anaconda Installation \u00b6 Download anaconda from below url https://www.anaconda.com/products/individual Install Anaconda bash Anaconda3-2021.05-Linux-x86_64.sh Downloading Apache Spark \u00b6 Go to https://spark.apache.org/downloads.html and download latest Spark file . Unzip it and Update Bash profile: tar -xzf spark-3.1.1-bin-hadoop2.7.tgz ` Configure Path nano ~/.bashrc ` Add below to bash profile export SPARK_HOME = /home/diwakar/Downloads/spark-3.1.1 export PATH = $SPARK_HOME /bin: $PATH Configure PySpark driver \u00b6 nano ~/.bashrc ` Add below to the environment variables export PYSPARK_DRIVER_PYTHON = jupyter export PYSPARK_DRIVER_PYTHON_OPTS = ' notebook Run \"pyspark\" in the window to start the jupyter notebook Run below code to test from pyspark.sql.types import StructType , StructField , FloatType , BooleanType from pyspark.sql.types import DoubleType , IntegerType , StringType import pyspark from pyspark import SQLContext conf = pyspark . SparkConf () sc = pyspark . SparkContext . getOrCreate ( conf = conf ) sqlcontext = SQLContext ( sc ) schema = StructType ([ StructField ( \"sales\" , IntegerType (), True ), StructField ( \"sales person\" , StringType (), True ) ]) data = ([( 10 , 'Walker' ), ( 20 , 'Stepher' ) ]) df = sqlcontext . createDataFrame ( data , schema = schema ) df . show ()","title":"Jupyter Notebook Integration"},{"location":"JupyterNotebookIntegration/#anaconda-installation","text":"Download anaconda from below url https://www.anaconda.com/products/individual Install Anaconda bash Anaconda3-2021.05-Linux-x86_64.sh","title":"Anaconda Installation"},{"location":"JupyterNotebookIntegration/#downloading-apache-spark","text":"Go to https://spark.apache.org/downloads.html and download latest Spark file . Unzip it and Update Bash profile: tar -xzf spark-3.1.1-bin-hadoop2.7.tgz ` Configure Path nano ~/.bashrc ` Add below to bash profile export SPARK_HOME = /home/diwakar/Downloads/spark-3.1.1 export PATH = $SPARK_HOME /bin: $PATH","title":"Downloading Apache Spark"},{"location":"JupyterNotebookIntegration/#configure-pyspark-driver","text":"nano ~/.bashrc ` Add below to the environment variables export PYSPARK_DRIVER_PYTHON = jupyter export PYSPARK_DRIVER_PYTHON_OPTS = ' notebook Run \"pyspark\" in the window to start the jupyter notebook Run below code to test from pyspark.sql.types import StructType , StructField , FloatType , BooleanType from pyspark.sql.types import DoubleType , IntegerType , StringType import pyspark from pyspark import SQLContext conf = pyspark . SparkConf () sc = pyspark . SparkContext . getOrCreate ( conf = conf ) sqlcontext = SQLContext ( sc ) schema = StructType ([ StructField ( \"sales\" , IntegerType (), True ), StructField ( \"sales person\" , StringType (), True ) ]) data = ([( 10 , 'Walker' ), ( 20 , 'Stepher' ) ]) df = sqlcontext . createDataFrame ( data , schema = schema ) df . show ()","title":"Configure PySpark driver"},{"location":"Overview/","text":"Introduction \u00b6 Why Spark? \u00b6 Hadoop was the first version of distributed computing Issues with Hadoop Not Unified You need to use Hive to write SQL , Mahout for ML , Zookeeper as cluster manager etc Hard to manage and administer Batch processing using Map Reduce Large Datasets were written to disk for subsequent stages of operation which meant high i/o. OK for batch processing but was slow while doing Machine learning / Streaming Spark was created in order to overcome these obstacles . What is Spark? \u00b6 Apache Spark is a unified engine designed for large-scale distributed data processing, on premises in data centers or in the cloud. Spark provides in-memory storage for intermediate computations, making it much faster than Hadoop Map Reduce. It incorporates libraries with composable APIs for machine learning (MLlib), SQL for interactive queries (Spark SQL), stream processing (Structured Streaming) for interacting with real-time data, and graph processing (GraphX). Spark\u2019s design philosophy centers around four key characteristics: Speed - The Spark framework is optimized to benefit from the cheap commodity hardware nowadays (CPU / RAM) - Spark builds its query as Directed acrylic graph (DAG) which constructs an efficient computational graph that can be decomposed into tasks and then can be executed in parallel across the workers in cluster . - As data is retained in memory with limited disk i/o it has a huge performance boost . Ease of use - All the high level abstractions such as data frames/datasets are built on top of simple logical structure called RDD. - This eventually leads to a simple programming model . Modularity - Supports Scala/Java/Python/R - All the libraries are well documented and are unified - We can write a single application to do ML/Streaming in one go . Extensibility - Focuses on parallel computation rather than storage - Decouple storage and computation - Supports many data sources and targets Apache Hadoop, Apache Cassandra, Apache HBase, MongoDB, Apache Hive, RDBMSs, and more\u2014and process it all in memory. - The community of Spark developers maintains a list of third-party Spark packages as part of the growing ecosystem Lets Revisit hadoop issues and compare those with Spark : \u00b6 Not Unified You need to use Hive to write SQL , Mahout for ML , Zookeeper as cluster manager etc -> Spark is unified with all ML, Streaming , SQL functionalities built in the core itself . Hard to manage and administer -> Spark is easy to Manage and administer Batch processing using Map Reduce -> Spark is must faster than traditional map reduce Large Datasets were written to disk for subsequent stages of operation which meant high i/o.-> Spark writes the data in memory making it much faster OK for batch processing but was slow while doing Machine learning / Streaming-> Spark is used for real time streaming and data processing . Apache Spark Components as a Unified Stack \u00b6 Spark has Four major components : Each of these components is separate in spark core engine So in whichever language you write the code Python/R the core decomposes it into highly compact byte code that is executed in worker's JVM across the cluster . Spark SQL: Works Well with Structured Data (RDBMS, csv, parquet , AVRO, ORC) Used to Read Data from RDBMS or structured data (csv, text, json etc) and create permanent / temporary tables in spark. Can be used to read data from dataframe . Useful to run SQL type queries Spark MLib Mlib provides many popular ML algorithms built on top of high level DataFrame based API to build models . These API allow to extract or transform features , build pipelines and persist models during deployment . Spark Structured Streaming Continuous streaming model where a stream of continuous streaming data can be consumed . Developers can treat these streams as tables and query them. Used to combine and react in real time to both static data and streaming data from engines like Apache Kafka , kinesis and other data sources . GraphX Library for manipulating Graphs and perform graph parallel computations . Apache Spark\u2019s Distributed Execution Model \u00b6 Spark Driver : Instantiates a spark Session Maintains information about the spark application. It\u2019s the heart of a Spark Application and maintains all relevant information during the lifetime of the application. It Communicates with cluster manager , requests resources from Cluster Manager for spark execution (JVM) , transforms spark operations into DAG, schedules operations and co-ordinates with executors . Spark Session The Spark application is controlled using SparkSession. Uniform conduit for all spark operations and data Used for creating JVM parameters , define Dataframe , Datasets , read data sources , access catalog metadata and issues SQL queries . Entry point for all Spark functionality Can be accessed using global variable spark or using sc . Cluster Manager Responsible for managing and allocating resources for the cluster Spark supports 4 cluster managers In built Standalone Apache Hadoop Yarn Apache Mesos Kubernetes Spark Executor Runs on each worker node in the cluster Communicates with driver program and execute tasks on the worker nodes They have 2 major responsibilities Execute Code assigned to it by the driver Reporting state of computation on that executor back to the driver node . Distributed Data and Partitions \u00b6 Actual Physical data is stored as partitions. It is a collection of rows that sit on one physical machine in your cluster. Spark treats each partition as a dataframe in memory Spark Executor reads the data from its closest partition Partitioning allows parallelism as Each core in partition is assigned to its own data partition to work with . If you have one partition, Spark will have a parallelism of only one, even if you have thousands of executors. If you have many partitions but only one executor, Spark will still have a parallelism of only one because there is only one computation resource. Partitioning allows efficient parallelism . In distributed environment spark executor reads data from the nearest partition allowing efficient parallelism . For example, this code snippet will break up the physical data stored across clusters into eight partitions, and each executor will get one or more partitions to read into its memory: log_df = spark.read.text(\"path_to_large_text_file\").repartition(8) print(log_df.rdd.getNumPartitions()) Understanding Spark Application Concepts \u00b6 Key terminologies Application : User Program consisting of driver and spark session . Spark Session : Entry point to interact with Spark functionality . Job : Parallel Computation consisting of many tasks Stage : Each Job is divided into smaller tasks called stages Task : Single unit of work Spark Application and Spark Session At the core of Spark Application is the spark driver program which creates a SparkSession object . While working with an interactive shell Spark session is created automatically Spark Jobs When we invoke commands through spark-shell the driver converts the spark application into various spark jobs which in turn converts each job into multiple DAG's . Spark Stages As park of DAG nodes stages re created based on the operations which needs to be performed . Spark Tasks Each stage is comprised of multiple tasks which is a unit of execution which gets federated across the spark executors . Each task maps to a single core and works on a single partition of data . For an executor with 16 core 16 or more tasks would run working on 16 or more partitions in parallel . Transformations Actions and Lazy Executions \u00b6 Transformations transform a spark dataframe into a new dataframe without altering the original data as it is immutable . E.g. a select() or a filter() command will not change the original dataframe but will return a new dataframe. Transformations are the core of how you express your business logic using Spark. All transformations are evaluated lazily aka the results are not computed immediately but are recorded which allows spark to rearrange the transformations , optimize them into stages for efficient execution . Lazy evaluation allows spark to record transformations until an action is invoked . Each transformation produces a new dataframe . Actions : Anything which triggers execution of a transformation like show, count etc. Kind of actions - Actions to view data in the console - Actions to collect data to native objects in the respective language - Actions to write to output data sources - Narrow and Wide Transformations \u00b6 Narrow Transformation : Where a single output is partition can be computed from a single input partition . E.g. filter(), contains() operate on single partition . Wide Transformations : Where a shuffle of partitions happens . E.g. groupby(), orderby() leads data to be read from many partitions combine them and then written to disk . An End-to-End Example \u00b6 We\u2019ll use Spark to analyze some flight data from the United States Bureau of Transportation statistics. Below is how data looks in csv file $ head / data / flight - data / csv / 2015 - summary . csv DEST_COUNTRY_NAME , ORIGIN_COUNTRY_NAME , count United States , Romania , 15 United States , Croatia , 1 United States , Ireland , 344 We will use below code to read the csv into a spark Dataframe flightData2015 = spark \\ . read \\ . option ( \"inferSchema\" , \"true\" ) \\ . option ( \"header\" , \"true\" ) \\ . csv ( \"/data/flight-data/csv/2015-summary.csv\" ) The Dataframe just created has a set of columns with an unspecified number of rows. The reason the number of rows is unspecified is because reading data is a transformation, and is therefore a lazy operation. Spark peeked at only a couple of rows of data to try to guess what types each column should be. Below provides an illustration of the CSV file being read into a Dataframe and then being converted into a local array or list of rows. If we perform the take action on the Dataframe, we will be able to see the same results that we saw before when we used the command line: flightData2015.take(3) Array([United States,Romania,15], [United States,Croatia... Let\u2019s specify some more transformations! Now, let\u2019s sort our data according to the count column, which is an integer type. Below figure illustrates this process. Also Sort does not modify the DataFrame. We use sort as a transformation that returns a new DataFrame by transforming the previous DataFrame. Nothing happens to the data when we call sort because it\u2019s just a transformation. However, we can see that Spark is building up a plan for how it will execute this across the cluster by looking at the explain plan. We can call explain on any DataFrame object to see the DataFrame\u2019s lineage (or how Spark will execute this query): flightData2015.sort(\"count\").explain() == Physical Plan == *Sort [count#195 ASC NULLS FIRST], true, 0 +- Exchange rangepartitioning(count#195 ASC NULLS FIRST, 200) +- *FileScan csv [DEST_COUNTRY_NAME#193,ORIGIN_COUNTRY_NAME#194,count#195] ... You can read explain plans from top to bottom, the top being the end result, and the bottom being the source(s) of data. In this case, take a look at the first keywords. You will see sort, exchange, and FileScan. That\u2019s because the sort of our data is actually a wide transformation because rows will need to be compared with one another. Now, just like we did before, we can specify an action to kick off this plan. However, before doing that, we\u2019re going to set a configuration. By default, when we perform a shuffle, Spark outputs 200 shuffle partitions. Let\u2019s set this value to 5 to reduce the number of the output partitions from the shuffle: spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\") flightData2015.sort(\"count\").take(2) ... Array([United States,Singapore,1], [Moldova,United States,1]) Below figure illustrates this operation. Notice that in addition to the logical transformations, we include the physical partition count, as well. The logical plan of transformations that we build up defines a lineage for the DataFrame so that at any given point in time, Spark knows how to recompute any partition by performing all of the operations it had before on the same input data. This sits at the heart of Spark\u2019s programming model\u2014functional programming where the same inputs always result in the same outputs when the transformations on that data stay constant. We do not manipulate the physical data; instead, we configure physical execution characteristics through things like the shuffle partitions parameter . We ended up with five output partitions because that\u2019s the value we specified in the shuffle partition. You can change this to help control the physical execution characteristics of your Spark jobs. In experimenting with different values, you should see drastically different runtimes. You can monitor the job progress by navigating to the Spark UI on port 4040 to see the physical and logical execution characteristics of your jobs.","title":"Overview"},{"location":"Overview/#introduction","text":"","title":"Introduction"},{"location":"Overview/#why-spark","text":"Hadoop was the first version of distributed computing Issues with Hadoop Not Unified You need to use Hive to write SQL , Mahout for ML , Zookeeper as cluster manager etc Hard to manage and administer Batch processing using Map Reduce Large Datasets were written to disk for subsequent stages of operation which meant high i/o. OK for batch processing but was slow while doing Machine learning / Streaming Spark was created in order to overcome these obstacles .","title":"Why Spark?"},{"location":"Overview/#what-is-spark","text":"Apache Spark is a unified engine designed for large-scale distributed data processing, on premises in data centers or in the cloud. Spark provides in-memory storage for intermediate computations, making it much faster than Hadoop Map Reduce. It incorporates libraries with composable APIs for machine learning (MLlib), SQL for interactive queries (Spark SQL), stream processing (Structured Streaming) for interacting with real-time data, and graph processing (GraphX). Spark\u2019s design philosophy centers around four key characteristics: Speed - The Spark framework is optimized to benefit from the cheap commodity hardware nowadays (CPU / RAM) - Spark builds its query as Directed acrylic graph (DAG) which constructs an efficient computational graph that can be decomposed into tasks and then can be executed in parallel across the workers in cluster . - As data is retained in memory with limited disk i/o it has a huge performance boost . Ease of use - All the high level abstractions such as data frames/datasets are built on top of simple logical structure called RDD. - This eventually leads to a simple programming model . Modularity - Supports Scala/Java/Python/R - All the libraries are well documented and are unified - We can write a single application to do ML/Streaming in one go . Extensibility - Focuses on parallel computation rather than storage - Decouple storage and computation - Supports many data sources and targets Apache Hadoop, Apache Cassandra, Apache HBase, MongoDB, Apache Hive, RDBMSs, and more\u2014and process it all in memory. - The community of Spark developers maintains a list of third-party Spark packages as part of the growing ecosystem","title":"What is Spark?"},{"location":"Overview/#lets-revisit-hadoop-issues-and-compare-those-with-spark","text":"Not Unified You need to use Hive to write SQL , Mahout for ML , Zookeeper as cluster manager etc -> Spark is unified with all ML, Streaming , SQL functionalities built in the core itself . Hard to manage and administer -> Spark is easy to Manage and administer Batch processing using Map Reduce -> Spark is must faster than traditional map reduce Large Datasets were written to disk for subsequent stages of operation which meant high i/o.-> Spark writes the data in memory making it much faster OK for batch processing but was slow while doing Machine learning / Streaming-> Spark is used for real time streaming and data processing .","title":"Lets Revisit hadoop issues and compare those with Spark :"},{"location":"Overview/#apache-spark-components-as-a-unified-stack","text":"Spark has Four major components : Each of these components is separate in spark core engine So in whichever language you write the code Python/R the core decomposes it into highly compact byte code that is executed in worker's JVM across the cluster . Spark SQL: Works Well with Structured Data (RDBMS, csv, parquet , AVRO, ORC) Used to Read Data from RDBMS or structured data (csv, text, json etc) and create permanent / temporary tables in spark. Can be used to read data from dataframe . Useful to run SQL type queries Spark MLib Mlib provides many popular ML algorithms built on top of high level DataFrame based API to build models . These API allow to extract or transform features , build pipelines and persist models during deployment . Spark Structured Streaming Continuous streaming model where a stream of continuous streaming data can be consumed . Developers can treat these streams as tables and query them. Used to combine and react in real time to both static data and streaming data from engines like Apache Kafka , kinesis and other data sources . GraphX Library for manipulating Graphs and perform graph parallel computations .","title":"Apache Spark Components as a Unified Stack"},{"location":"Overview/#apache-sparks-distributed-execution-model","text":"Spark Driver : Instantiates a spark Session Maintains information about the spark application. It\u2019s the heart of a Spark Application and maintains all relevant information during the lifetime of the application. It Communicates with cluster manager , requests resources from Cluster Manager for spark execution (JVM) , transforms spark operations into DAG, schedules operations and co-ordinates with executors . Spark Session The Spark application is controlled using SparkSession. Uniform conduit for all spark operations and data Used for creating JVM parameters , define Dataframe , Datasets , read data sources , access catalog metadata and issues SQL queries . Entry point for all Spark functionality Can be accessed using global variable spark or using sc . Cluster Manager Responsible for managing and allocating resources for the cluster Spark supports 4 cluster managers In built Standalone Apache Hadoop Yarn Apache Mesos Kubernetes Spark Executor Runs on each worker node in the cluster Communicates with driver program and execute tasks on the worker nodes They have 2 major responsibilities Execute Code assigned to it by the driver Reporting state of computation on that executor back to the driver node .","title":"Apache Spark\u2019s Distributed Execution Model"},{"location":"Overview/#distributed-data-and-partitions","text":"Actual Physical data is stored as partitions. It is a collection of rows that sit on one physical machine in your cluster. Spark treats each partition as a dataframe in memory Spark Executor reads the data from its closest partition Partitioning allows parallelism as Each core in partition is assigned to its own data partition to work with . If you have one partition, Spark will have a parallelism of only one, even if you have thousands of executors. If you have many partitions but only one executor, Spark will still have a parallelism of only one because there is only one computation resource. Partitioning allows efficient parallelism . In distributed environment spark executor reads data from the nearest partition allowing efficient parallelism . For example, this code snippet will break up the physical data stored across clusters into eight partitions, and each executor will get one or more partitions to read into its memory: log_df = spark.read.text(\"path_to_large_text_file\").repartition(8) print(log_df.rdd.getNumPartitions())","title":"Distributed Data and Partitions"},{"location":"Overview/#understanding-spark-application-concepts","text":"Key terminologies Application : User Program consisting of driver and spark session . Spark Session : Entry point to interact with Spark functionality . Job : Parallel Computation consisting of many tasks Stage : Each Job is divided into smaller tasks called stages Task : Single unit of work Spark Application and Spark Session At the core of Spark Application is the spark driver program which creates a SparkSession object . While working with an interactive shell Spark session is created automatically Spark Jobs When we invoke commands through spark-shell the driver converts the spark application into various spark jobs which in turn converts each job into multiple DAG's . Spark Stages As park of DAG nodes stages re created based on the operations which needs to be performed . Spark Tasks Each stage is comprised of multiple tasks which is a unit of execution which gets federated across the spark executors . Each task maps to a single core and works on a single partition of data . For an executor with 16 core 16 or more tasks would run working on 16 or more partitions in parallel .","title":"Understanding Spark Application Concepts"},{"location":"Overview/#transformations-actions-and-lazy-executions","text":"Transformations transform a spark dataframe into a new dataframe without altering the original data as it is immutable . E.g. a select() or a filter() command will not change the original dataframe but will return a new dataframe. Transformations are the core of how you express your business logic using Spark. All transformations are evaluated lazily aka the results are not computed immediately but are recorded which allows spark to rearrange the transformations , optimize them into stages for efficient execution . Lazy evaluation allows spark to record transformations until an action is invoked . Each transformation produces a new dataframe . Actions : Anything which triggers execution of a transformation like show, count etc. Kind of actions - Actions to view data in the console - Actions to collect data to native objects in the respective language - Actions to write to output data sources -","title":"Transformations Actions and Lazy Executions"},{"location":"Overview/#narrow-and-wide-transformations","text":"Narrow Transformation : Where a single output is partition can be computed from a single input partition . E.g. filter(), contains() operate on single partition . Wide Transformations : Where a shuffle of partitions happens . E.g. groupby(), orderby() leads data to be read from many partitions combine them and then written to disk .","title":"Narrow and Wide Transformations"},{"location":"Overview/#an-end-to-end-example","text":"We\u2019ll use Spark to analyze some flight data from the United States Bureau of Transportation statistics. Below is how data looks in csv file $ head / data / flight - data / csv / 2015 - summary . csv DEST_COUNTRY_NAME , ORIGIN_COUNTRY_NAME , count United States , Romania , 15 United States , Croatia , 1 United States , Ireland , 344 We will use below code to read the csv into a spark Dataframe flightData2015 = spark \\ . read \\ . option ( \"inferSchema\" , \"true\" ) \\ . option ( \"header\" , \"true\" ) \\ . csv ( \"/data/flight-data/csv/2015-summary.csv\" ) The Dataframe just created has a set of columns with an unspecified number of rows. The reason the number of rows is unspecified is because reading data is a transformation, and is therefore a lazy operation. Spark peeked at only a couple of rows of data to try to guess what types each column should be. Below provides an illustration of the CSV file being read into a Dataframe and then being converted into a local array or list of rows. If we perform the take action on the Dataframe, we will be able to see the same results that we saw before when we used the command line: flightData2015.take(3) Array([United States,Romania,15], [United States,Croatia... Let\u2019s specify some more transformations! Now, let\u2019s sort our data according to the count column, which is an integer type. Below figure illustrates this process. Also Sort does not modify the DataFrame. We use sort as a transformation that returns a new DataFrame by transforming the previous DataFrame. Nothing happens to the data when we call sort because it\u2019s just a transformation. However, we can see that Spark is building up a plan for how it will execute this across the cluster by looking at the explain plan. We can call explain on any DataFrame object to see the DataFrame\u2019s lineage (or how Spark will execute this query): flightData2015.sort(\"count\").explain() == Physical Plan == *Sort [count#195 ASC NULLS FIRST], true, 0 +- Exchange rangepartitioning(count#195 ASC NULLS FIRST, 200) +- *FileScan csv [DEST_COUNTRY_NAME#193,ORIGIN_COUNTRY_NAME#194,count#195] ... You can read explain plans from top to bottom, the top being the end result, and the bottom being the source(s) of data. In this case, take a look at the first keywords. You will see sort, exchange, and FileScan. That\u2019s because the sort of our data is actually a wide transformation because rows will need to be compared with one another. Now, just like we did before, we can specify an action to kick off this plan. However, before doing that, we\u2019re going to set a configuration. By default, when we perform a shuffle, Spark outputs 200 shuffle partitions. Let\u2019s set this value to 5 to reduce the number of the output partitions from the shuffle: spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\") flightData2015.sort(\"count\").take(2) ... Array([United States,Singapore,1], [Moldova,United States,1]) Below figure illustrates this operation. Notice that in addition to the logical transformations, we include the physical partition count, as well. The logical plan of transformations that we build up defines a lineage for the DataFrame so that at any given point in time, Spark knows how to recompute any partition by performing all of the operations it had before on the same input data. This sits at the heart of Spark\u2019s programming model\u2014functional programming where the same inputs always result in the same outputs when the transformations on that data stay constant. We do not manipulate the physical data; instead, we configure physical execution characteristics through things like the shuffle partitions parameter . We ended up with five output partitions because that\u2019s the value we specified in the shuffle partition. You can change this to help control the physical execution characteristics of your Spark jobs. In experimenting with different values, you should see drastically different runtimes. You can monitor the job progress by navigating to the Spark UI on port 4040 to see the physical and logical execution characteristics of your jobs.","title":"An End-to-End Example"},{"location":"about/","text":"\"# docs\"","title":"About"}]}